{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-01-20T08:57:47.173317Z",
     "start_time": "2025-01-20T08:57:45.358461Z"
    }
   },
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "# @Time    : 1/20/2025 2:31 PM\n",
    "# @FileName: test7.py\n",
    "# @Software: PyCharm\n",
    "from owlready2 import *\n",
    "import pyAgrum as gum\n",
    "# import pyAgrum.lib.notebook as gnb\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import truncnorm, norm\n",
    "import pyAgrum.lib.image as gumimage\n",
    "import random\n",
    "import graphviz\n",
    "import openpyxl\n",
    "import ast\n",
    "\n",
    "# 1. 读入OWL文件\n",
    "#onto = get_ontology(r\"C:\\Users\\Tsing_loong\\Desktop\\Work Section\\推演代码合成\\推演代码合成\\ScenarioOntology.owl\").load()\n",
    "onto = get_ontology(r\"D:\\PythonProjects\\AcademicTool_PySide\\data\\sysml2\\21\\owl\\Scenario.owl\").load()\n",
    "# 准备存储信息的数据结构\n",
    "data_properties_info = []\n",
    "object_properties_info = []\n",
    "bn = gum.BayesNet('ScenarioDeductionBN')\n",
    "# 2. 提取dataproperty元素的名称、domain和range\n",
    "\n",
    "for dps in onto.data_properties():\n",
    "    data_properties_info.append({\"name\": dps.name, \"domain\": dps.domain[0].name, \"range\": dps.range[0]})\n",
    "\n",
    "# 将resourseType具体转化为 4 种\n",
    "\n",
    "for item in data_properties_info:\n",
    "    if item['name'] == 'resourceType':\n",
    "        info = item\n",
    "        break\n",
    "for resource in ['AidResource', 'TowResource', 'FirefightingResource', 'RescueResource']:\n",
    "    item = info.copy()\n",
    "    item['name'] = resource\n",
    "    data_properties_info.append(item)\n",
    "data_properties_info = [item for item in data_properties_info if item['name'] != 'resourceType']\n",
    "\n",
    "print(data_properties_info)\n",
    "\n",
    "# 创建一个字典，键为factor nodes，值为其对应的状态变量\n",
    "dict_factor_node_values = {\n",
    "    'roadPassibility': ['Passable', 'Impassable'],\n",
    "    'emergencyType': ['Vehicle_Self_Accident', 'Vehicle_to_Fixed_Object_Accident', 'Collision_Accident'],\n",
    "    'roadLoss': ['Loss', 'Not_Loss'],\n",
    "    'casualties': ['Casualties', 'No_Casualties'],\n",
    "    #    'resourceType': ['Aid_Resource', 'Tow_Resource', 'Firefighting_Resource', 'Rescue_Resource'],\n",
    "    'AidResource': ['Not_Used', 'Used'],\n",
    "    'TowResource': ['Not_Used', 'Used'],\n",
    "    'FirefightingResource': ['Not_Used', 'Used'],\n",
    "    'RescueResource': ['Not_Used', 'Used'],\n",
    "    'emergencyPeriod': ['Early_Morning', 'Morning', 'Afternoon', 'Evening'],\n",
    "    'responseDuration': ['0-15min', '15-30min', '30-60min', '60min+'],\n",
    "    'disposalDuration': ['0-15min', '15-30min', '30-60min', '60min+']\n",
    "}\n",
    "# 创建一个字典，键为capacity nodes，值为其对应的factor nodes\n",
    "dict_factor_capacity = {\n",
    "    'AbsorptionScenario': ['roadPassibility', 'roadLoss'],\n",
    "    'AdaptionScenario': ['emergencyType', 'emergencyPeriod'],\n",
    "    'RecoveryScenario': ['AidResource', 'TowResource', 'FirefightingResource', 'RescueResource', 'responseDuration',\n",
    "                         'disposalDuration', 'casualties']\n",
    "}\n",
    "\n",
    "\n",
    "def find_capacity_by_factor(factor, c_f_dict):\n",
    "    matching_keys = []\n",
    "    for key, values in c_f_dict.items():\n",
    "        if factor in values:\n",
    "            matching_keys.append(key)\n",
    "    return matching_keys\n",
    "\n",
    "\n",
    "# 3. ontology转换为bn\n",
    "classes = [cls.name for cls in onto.classes()]\n",
    "#print(onto.classes())\n",
    "node_resilience = bn.add(gum.LabelizedVariable(\"ScenarioResilience\", 'resilienceLevel', 2))\n",
    "\n",
    "for cls in onto.classes():\n",
    "    #print(cls)\n",
    "    if cls.name == \"Scenario\":\n",
    "        capacities = list(cls.subclasses())\n",
    "        for capacity in capacities:\n",
    "            capacityName = capacity.name.replace(\"Scenario\", \"\") + 'Capacity'\n",
    "            bn.add(gum.LabelizedVariable(capacityName, 'capacityLevel', 2))\n",
    "            bn.addArc(capacityName, \"ScenarioResilience\")\n",
    "    elif cls.name == \"ResilienceInfluentialFactors\":\n",
    "        factorProperties = list(cls.subclasses())\n",
    "        for pro in factorProperties:\n",
    "            for dp in data_properties_info:\n",
    "                #                 print(pro.name)\n",
    "                if dp['domain'] == pro.name:\n",
    "                    #print(\"dict_factor_node_values:\", dict_factor_node_values)\n",
    "                    #print(\"dp['name']:\", dp['name'])\n",
    "                    # if dp['name'] not in dict_factor_node_values:\n",
    "                    #     print(f\"Warning: {dp['name']} not found in dict_factor_node_values\")\n",
    "                    #     continue  # 跳过这个数据属性\n",
    "                    bn.add(\n",
    "                        gum.LabelizedVariable(dp['name'], 'factorLevel', len(dict_factor_node_values.get(dp['name']))))\n",
    "                    result = find_capacity_by_factor(dp['name'], dict_factor_capacity)\n",
    "                    #                     print(f\"'{dp['name']}' 在字典中的对应键为: {', '.join(result)}\")\n",
    "                    for arc in result:\n",
    "                        capacityName = arc.replace(\"Scenario\", \"\") + 'Capacity'\n",
    "                        bn.addArc(dp['name'], capacityName)\n",
    "\n",
    "\n",
    "# print(bn)\n",
    "\n",
    "# dot_structure = bn.toDot()\n",
    "# print(dot_structure)\n",
    "\n",
    "def calculate_DiscreteVariable_Prior(node_name, data):\n",
    "    category_counts = data.value_counts()\n",
    "    # 计算总样本数\n",
    "    total_samples = category_counts.sum()\n",
    "\n",
    "    # 计算每个类别的概率分布\n",
    "    category_probabilities = category_counts / total_samples\n",
    "    #     print(category_probabilities.tolist())\n",
    "    bn.cpt(node_name).fillWith(category_probabilities.tolist())\n",
    "\n",
    "\n",
    "def calculate_ContinuousVariable_Thorm(data):\n",
    "    # 计算均值和标准差\n",
    "    mu, std = norm.fit(data)\n",
    "    # 打印拟合的均值和标准差\n",
    "    #     print(f\"Fitted Mean (μ): {mu:.2f}\")\n",
    "    #     print(f\"Fitted Standard Deviation (σ): {std:.2f}\")\n",
    "\n",
    "    lower_bound = np.percentile(data, 5)  # 5th percentile\n",
    "    upper_bound = np.percentile(data, 95)  # 95th percentile\n",
    "    #     print(f\"Lower Bound: {lower_bound}, Upper Bound: {upper_bound}\")\n",
    "    # Calculate the z-scores for the truncation bounds\n",
    "    a = (lower_bound - mu) / std  # Lower bound in terms of z-score\n",
    "    b = (upper_bound - mu) / std  # Upper bound in terms of z-score\n",
    "\n",
    "    # Create a truncated normal distribution with the calculated parameters\n",
    "    truncated_normal = truncnorm(a, b, loc=mu, scale=std)\n",
    "    return truncated_normal\n",
    "\n",
    "\n",
    "def discretize_calculate_probabilities(x, truncated_normal):\n",
    "    prob = truncated_normal.cdf(x)\n",
    "    #     print(f\"P(X < {x}) for a truncated normal distribution is: {prob}\")\n",
    "    return (prob)\n",
    "\n",
    "\n",
    "def draw_thorm(truncated_normal, data):\n",
    "    mu, std = norm.fit(data)\n",
    "    lower_bound = np.percentile(data, 5)  # 5th percentile\n",
    "    upper_bound = np.percentile(data, 95)  # 95th percentile\n",
    "    sampled_values = truncated_normal.rvs(size=1000)\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.hist(sampled_values, bins=30, density=True, alpha=0.6, color='g', label='Data Histogram')\n",
    "\n",
    "    # 绘制截断正态分布曲线\n",
    "    x = np.linspace(lower_bound, upper_bound, 1000)\n",
    "    p = truncated_normal.pdf(x)\n",
    "    plt.plot(x, p, 'k', linewidth=2, label=f'Truncated Normal Fit (μ={mu:.2f}, σ={std:.2f})')\n",
    "\n",
    "    # 添加标签和图例\n",
    "    plt.title('Truncated Normal Distribution Fit')\n",
    "    plt.xlabel('Continuous Variable')\n",
    "    plt.ylabel('Density')\n",
    "    plt.legend()\n",
    "\n",
    "    # 显示图形\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "data = pd.read_excel(r\"C:\\Users\\Tsing_loong\\Desktop\\Work Section\\推演代码合成\\推演代码合成\\prior prob test.xlsx\")\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# 4.设置根节点的先验概率\n",
    "for node in bn.nodes():\n",
    "    node_name = bn.variable(node).name()\n",
    "    #     print(node_name != \"emergencyPeriod\")\n",
    "    if (len(bn.parents(node)) == 0):\n",
    "        if node_name not in [\"disposalDuration\", \"responseDuration\"]:\n",
    "            #         print(node_name)\n",
    "            calculate_DiscreteVariable_Prior(node_name, df[node_name])\n",
    "        elif node_name in [\"disposalDuration\", \"responseDuration\"]:\n",
    "            truncated_normal = calculate_ContinuousVariable_Thorm(df[node_name])\n",
    "            prob_0_15 = discretize_calculate_probabilities(15, truncated_normal)\n",
    "            prob_15_30 = discretize_calculate_probabilities(30, truncated_normal) - prob_0_15\n",
    "            prob_30_60 = discretize_calculate_probabilities(60, truncated_normal) - discretize_calculate_probabilities(\n",
    "                30, truncated_normal)\n",
    "            prob_morethan_60 = 1 - discretize_calculate_probabilities(60, truncated_normal)\n",
    "            #             print(prob_0_15,prob_15_30,prob_30_60,prob_morethan_60)\n",
    "            bn.cpt(node_name).fillWith([prob_0_15, prob_15_30, prob_30_60, prob_morethan_60])\n",
    "\n",
    "# 定义评估结果与模糊数的映射关系\n",
    "mapping_evaluation_fuzzy = {\n",
    "    'VL': (0.0, 0.0, 0.1, 0.2),\n",
    "    'L': (0.1, 0.2, 0.2, 0.3),\n",
    "    'M': (0.2, 0.3, 0.4, 0.5),\n",
    "    'H': (0.5, 0.6, 0.7, 0.8),\n",
    "    'VH': (0.8, 0.9, 1.0, 1.0)\n",
    "}\n",
    "\n",
    "df = pd.read_excel(r\"C:\\Users\\Tsing_loong\\Desktop\\Work Section\\推演代码合成\\推演代码合成\\expert estimation test.xlsx\")\n",
    "\n",
    "\n",
    "# print(df)\n",
    "\n",
    "def calculate_fuzzy(row):\n",
    "    fuzzy_numbers = [mapping_evaluation_fuzzy[r] for r in row]\n",
    "    return fuzzy_numbers\n",
    "\n",
    "\n",
    "df['fuzzy'] = df.iloc[:, -7:].apply(calculate_fuzzy, axis=1)\n",
    "df['Condition'] = df['Condition'].tolist()\n",
    "\n",
    "\n",
    "# print(df)\n",
    "\n",
    "def calculate_similarity(row):\n",
    "    # 初始化结果列表，用于存储每个元素与其他元素差值的平均值\n",
    "    ss = []\n",
    "    avg_similarity = []\n",
    "    relative_similarity = []\n",
    "    # 获取数组的长度\n",
    "    n = len(row)\n",
    "    k = list(range(n))\n",
    "\n",
    "    # 遍历数组中的每个元素（元组）\n",
    "    for i in range(n):\n",
    "        temp = []\n",
    "        k_filtered = [element for element in k if element != i]\n",
    "        for j in k_filtered:\n",
    "            differences_sum = 1 - (sum(abs(a - b) for a, b in zip(row[i], row[j]))) / len(row[i])\n",
    "            temp.append(differences_sum)\n",
    "        avg_similarity.append(sum(temp) / len(temp))\n",
    "        ss.append(temp)\n",
    "        relative_similarity.append(avg_similarity[i] / sum(avg_similarity))\n",
    "    #     print(avg_similarity)\n",
    "    #     print(relative_similarity)\n",
    "\n",
    "    return ss, avg_similarity, relative_similarity\n",
    "\n",
    "\n",
    "df_expert = pd.read_excel(r\"C:\\Users\\Tsing_loong\\Desktop\\Work Section\\推演代码合成\\推演代码合成\\expertInfo.xlsx\")\n",
    "scores = df_expert.iloc[:, 1:].sum(axis=1)\n",
    "total_score = scores.sum()\n",
    "weight_list = list(scores / total_score)\n",
    "print(weight_list)\n",
    "\n",
    "\n",
    "def calculate_aggrated_fuzzy(row):\n",
    "    b = 0.5\n",
    "    aggrated_fuzzy = [0.0, 0.0, 0.0, 0.0]  # 四个位置对应四个加权平均数\n",
    "    global weight_list\n",
    "\n",
    "    ss, avg_s, rela_s = calculate_similarity(row)\n",
    "    cc = [(b * weight_list[i] + (1 - b) * rela_s[i]) for i in range(len(rela_s))]\n",
    "    weight = [i / sum(cc) for i in cc]  # 归一化，使和为1\n",
    "\n",
    "    # 计算加权平均数\n",
    "    for weights, tup in zip(weight, row):\n",
    "        #         print(weights)\n",
    "        for i in range(len(tup)):\n",
    "            #             print('i:',tup[i])\n",
    "            #             print(tup[i] * weights)\n",
    "            aggrated_fuzzy[i] += tup[i] * weights\n",
    "\n",
    "    a1, a2, a3, a4 = aggrated_fuzzy\n",
    "    defuzzified_possibility = ((a4 + a3) ** 2 - a4 * a3 - (a1 + a2) ** 2 + a1 * a2) / (3 * (a4 + a3 - a2 - a1))\n",
    "\n",
    "    return defuzzified_possibility\n",
    "\n",
    "\n",
    "# 条件概率填充到 df\n",
    "conditonProbability = []\n",
    "for row in df['fuzzy']:\n",
    "    conditonProbability.append(calculate_aggrated_fuzzy(row))\n",
    "#    print(calculate_aggrated_fuzzy(row))\n",
    "df['conditonProbability'] = conditonProbability\n",
    "\n",
    "\n",
    "# 条件概率归一化\n",
    "def normalize(group):\n",
    "    s = group['conditonProbability'].sum()\n",
    "    group['conditonProbability'] = group['conditonProbability'] / s\n",
    "    return group\n",
    "\n",
    "\n",
    "normalized_prob = (df.groupby(['Node', 'Condition']).apply(normalize))['conditonProbability']\n",
    "df['conditonProbability'] = normalized_prob.tolist()\n",
    "# print(df)\n",
    "\n",
    "# 将 Condition 和 State 结合为 cpt 填充时的索引列表\n",
    "index = []\n",
    "for i, row in df.iterrows():\n",
    "    sublist = ast.literal_eval(row['Condition'])\n",
    "    sublist.append(row['State'])\n",
    "    index.append(sublist)\n",
    "# print(index)\n",
    "\n",
    "# 创建 Potential 对象字典\n",
    "cpt = {'AbsorptionCapacity': bn.cpt('AbsorptionCapacity'), 'AdaptionCapacity': bn.cpt('AdaptionCapacity'),\n",
    "       'RecoveryCapacity': bn.cpt('RecoveryCapacity'), 'ScenarioResilience': bn.cpt('ScenarioResilience')}\n",
    "# 条件概率填充\n",
    "for i, row in df.iterrows():\n",
    "    (cpt[row['Node']])[ast.literal_eval(','.join(str(x) for x in index[i]))] = row['conditonProbability']\n",
    "# 将 Potential 对象赋值给 cpt\n",
    "for i in ['AbsorptionCapacity', 'AdaptionCapacity', 'RecoveryCapacity', 'ScenarioResilience']:\n",
    "    bn.cpt(i).fillWith(cpt[i])\n",
    "# print(bn.cpt(\"AbsorptionCapacity\"))\n",
    "\n",
    "# 6.证据更新\n",
    "# onto = get_ontology(\"file://ScenarioOntology.owl\")\n",
    "# onto.load()\n",
    "# print(list(onto.data_properties()))\n",
    "# print(list(onto.properties()))\n",
    "# with onto:\n",
    "#    rule1 = Imp()\n",
    "#     rule1.set_as_rule(\"\"\"\"\"\")\n",
    "# 当保存预案时，更新预案实例\n",
    "# 执行swrl推理\n",
    "# sync_reasoner_pellet(infer_property_values = True, infer_data_property_values = True)\n",
    "\n",
    "# 7.执行推理：使用lazyPropagation推理引擎精确推断\n",
    "ie = gum.LazyPropagation(bn)\n",
    "# 没有新数据的推断\n",
    "ie.makeInference()\n",
    "print(ie.posterior(\"ScenarioResilience\"))\n",
    "print(ie.posterior(\"RecoveryCapacity\"))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'name': 'casualties', 'domain': 'SafetyFactors', 'range': <class 'bool'>}, {'name': 'emergencyType', 'domain': 'SafetyFactors', 'range': <class 'str'>}, {'name': 'roadPassibility', 'domain': 'FunctionFactors', 'range': <class 'bool'>}, {'name': 'roadLoss', 'domain': 'EconomicFactors', 'range': <class 'bool'>}, {'name': 'disposalDuration', 'domain': 'TimeFactors', 'range': <class 'str'>}, {'name': 'emergencyPeriod', 'domain': 'TimeFactors', 'range': <class 'str'>}, {'name': 'responseDuration', 'domain': 'TimeFactors', 'range': <class 'str'>}, {'name': 'AidResource', 'domain': 'EconomicFactors', 'range': <class 'str'>}, {'name': 'TowResource', 'domain': 'EconomicFactors', 'range': <class 'str'>}, {'name': 'FirefightingResource', 'domain': 'EconomicFactors', 'range': <class 'str'>}, {'name': 'RescueResource', 'domain': 'EconomicFactors', 'range': <class 'str'>}]\n",
      "[0.10619469026548672, 0.13274336283185842, 0.1415929203539823, 0.168141592920354, 0.1504424778761062, 0.13274336283185842, 0.168141592920354]\n",
      "\n",
      "  ScenarioResilienc|\n",
      "0        |1        |\n",
      "---------|---------|\n",
      " 0.6217  | 0.3783  |\n",
      "\n",
      "\n",
      "  RecoveryCapacity |\n",
      "0        |1        |\n",
      "---------|---------|\n",
      " 0.6268  | 0.3732  |\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tsing_loong\\AppData\\Local\\Temp\\ipykernel_28612\\3139098650.py:289: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  normalized_prob = (df.groupby(['Node', 'Condition']).apply(normalize))['conditonProbability']\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-20T08:58:00.531409Z",
     "start_time": "2025-01-20T08:58:00.512394Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "# 有新数据的推断\n",
    "ie.setEvidence({'AidResource': 0})\n",
    "ie.makeInference()\n",
    "print(ie.posterior(\"ScenarioResilience\"))\n",
    "print(ie.posterior(\"RecoveryCapacity\"))"
   ],
   "id": "af7c05b6ea754b91",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  ScenarioResilienc|\n",
      "0        |1        |\n",
      "---------|---------|\n",
      " 0.6219  | 0.3781  |\n",
      "\n",
      "\n",
      "  RecoveryCapacity |\n",
      "0        |1        |\n",
      "---------|---------|\n",
      " 0.6323  | 0.3677  |\n",
      "\n"
     ]
    }
   ],
   "execution_count": 2
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
